{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4125,"status":"ok","timestamp":1671545868015,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"gAPixMnltm20","outputId":"044389c2-69e2-4ba1-c6ff-93d53a466511"},"outputs":[],"source":["!pip install sentencepiece\n","!pip3 install torch torchvision torchaudio\n","!pip install pandas\n","!pip install numpy\n","!pip install scikit-learn\n","!pip install transformers\n","!pip install datasets\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import gc\n","\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","# set a seed value\n","torch.manual_seed(555)\n","\n","from sklearn.utils import shuffle\n","from sklearn.metrics import roc_auc_score, accuracy_score\n","\n","import transformers\n","from transformers import AutoTokenizer, XLMRobertaForSequenceClassification\n","from transformers import AdamW\n","\n","from datasets import load_dataset\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","print(torch.__version__)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Settings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1671545868016,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"7jv2ekewtm22","outputId":"189548aa-4a3b-4b97-9206-33fc3b76e301"},"outputs":[],"source":["MODEL_TYPE = 'joeddav/xlm-roberta-large-xnli'\n","\n","\n","L_RATE = 1e-5\n","MAX_LEN = 256\n","\n","NUM_EPOCHS = 5\n","BATCH_SIZE = 32\n","NUM_CORES = os.cpu_count()\n","\n","NUM_CORES"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1671545868016,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"P_vSpen8tm22","outputId":"dae87a37-26d5-4fe5-87bd-2b175106e3d9"},"outputs":[],"source":["# move to GPU\n","device = torch.device('cuda:0')\n","device"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2072,"status":"ok","timestamp":1671545870079,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"A2PZE-SvuDT5","outputId":"97fa6978-deea-46cc-d82b-b19fa41c2694"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1671545870079,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"6XvCwc2vtm23","outputId":"2c52f2e2-27aa-4504-9045-85d3b008f437"},"outputs":[],"source":["df_train=pd.read_csv(\"drive/MyDrive/NLP_Project/data/train.csv\")\n","df_val=pd.read_csv(\"drive/MyDrive/NLP_Project/data/valid.csv\")\n","df_test=pd.read_csv(\"drive/MyDrive/NLP_Project/data/test.csv\")\n","\n","print(df_train.shape)"]},{"cell_type":"markdown","metadata":{"id":"eFWJzpt8nSfR"},"source":["### Load mnli data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":273,"referenced_widgets":["3b1bacb8419c498d854d020c79847cc6","c83fce16b6214a5bbc79385f03961e5d","e9e4f6ab9e364a2d9a61b784ecf52806","8b2b05f0e4a743e7b67fd6e817eb3a5f","7fb4ad8c04c942ee81a4c462e29ba9f0","59db2e01f17f4736b16b2a4d61253d59","c361381b324841e694dfcc68df691998","25f3146c716d4c939540347cbf5eb73e","1c09cfb3782c4d51a3a65e88f23c664f","3d59e937083441c5b1f2592cd75ca9d4","dffd536873e04a3b915f816b2a322d66"]},"executionInfo":{"elapsed":18567,"status":"ok","timestamp":1671545888643,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"2cl52eXqjO4A","outputId":"bb68e881-82c5-4d4e-819e-36006ae9a1a1"},"outputs":[],"source":["mnli = load_dataset('glue', 'mnli')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1671545888952,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"xG4J4J8l2aXZ","outputId":"ae8ad24e-b94a-430a-ba7c-08ccfd22cf5b"},"outputs":[],"source":["df_mnli = pd.DataFrame.from_dict(mnli[\"validation_matched\"])\n","df_mnli.drop(columns=['idx'], inplace=True)\n","df_mnli.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1671545888952,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"A4RsW1R7nWrP"},"outputs":[],"source":["df_mnli['label'] = df_mnli['label'].replace([0, 2], [2, 0])       # because 0, 2 are changed in training of xlm_roberta_large_xnli\n","df_val['label'] = df_val['label'].replace([0, 2], [2, 0])                     # because 0, 2 are changed in training of xlm_roberta_large_xnli"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1671545890159,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"WGYLtM_ftm23"},"outputs":[],"source":["df_val = df_val.reset_index(drop=True)\n","df_train_mnli = df_mnli.reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{"id":"LpWdAlqhnb6C"},"source":["### Load Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1209,"status":"ok","timestamp":1671545890158,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"o5Srx7KAtm23","outputId":"cd7db358-2a80-4ea5-e7bd-d6a800130666"},"outputs":[],"source":["print('Loading Roberta tokenizer...')\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Create Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1671545890159,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"Swhu9epptm24"},"outputs":[],"source":["class CompDataset(Dataset):\n","\n","    def __init__(self, df):\n","        self.df_data = df\n","\n","\n","\n","    def __getitem__(self, index):\n","\n","        # get the sentence from the dataframe\n","        sentence1 = self.df_data.loc[index, 'premise']\n","        sentence2 = self.df_data.loc[index, 'hypothesis']\n","\n","        # Process the sentence\n","        # ---------------------\n","\n","        encoded_dict = tokenizer.encode_plus(\n","                    sentence1, sentence2,           # Sentences to encode.\n","                    add_special_tokens = True,      # Add the special tokens.\n","                    max_length = MAX_LEN,\n","                    truncation=True,           # Pad & truncate all sentences.\n","                    pad_to_max_length = True,\n","                    return_attention_mask = True,   # Construct attn. masks.\n","                    return_tensors = 'pt',          # Return pytorch tensors.\n","               )\n","        \n","        # These are torch tensors.\n","        padded_token_list = encoded_dict['input_ids'][0]\n","        att_mask = encoded_dict['attention_mask'][0]\n","        \n","        # Convert the target to a torch tensor\n","        target = torch.tensor(self.df_data.loc[index, 'label'])\n","\n","        sample = (padded_token_list, att_mask, target)\n","\n","\n","        return sample\n","\n","\n","    def __len__(self):\n","        return len(self.df_data)\n","    \n","\n","class TestDataset(Dataset):\n","\n","    def __init__(self, df):\n","        self.df_data = df\n","\n","\n","\n","    def __getitem__(self, index):\n","\n","        # get the sentence from the dataframe\n","        sentence1 = self.df_data.loc[index, 'premise']\n","        sentence2 = self.df_data.loc[index, 'hypothesis']\n","\n","        # Process the sentence\n","        # ---------------------\n","\n","        encoded_dict = tokenizer.encode_plus(\n","                    sentence1, sentence2,           # Sentence to encode.\n","                    add_special_tokens = True,      # Add the special tokens.\n","                    max_length = MAX_LEN,           # Pad & truncate all sentences.\n","                    truncation=True,           # Pad & truncate all sentences.\n","                    pad_to_max_length = True,\n","                    return_attention_mask = True,   # Construct attn. masks.\n","                    return_tensors = 'pt',          # Return pytorch tensors.\n","               )\n","        \n","        # These are torch tensors.\n","        padded_token_list = encoded_dict['input_ids'][0]\n","        att_mask = encoded_dict['attention_mask'][0]\n","        \n","               \n","\n","        sample = (padded_token_list, att_mask)\n","\n","\n","        return sample\n","\n","\n","    def __len__(self):\n","        return len(self.df_data)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12587,"status":"ok","timestamp":1671545902740,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"ofiWSvAdtm26","outputId":"e2a9e962-137d-4fa6-83cd-e0e6a8b36a20"},"outputs":[],"source":["from transformers import RobertaForSequenceClassification\n","\n","model = RobertaForSequenceClassification.from_pretrained(\n","    MODEL_TYPE, \n","    num_labels = 3, # The number of output labels. 2 for binary classification.\n",")\n","\n","# Send the model to the device.\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1671545902740,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"gdtKOQrDtm28"},"outputs":[],"source":["# Define the optimizer\n","optimizer = AdamW(model.parameters(),\n","              lr = L_RATE, \n","              eps = 1e-8 \n","            )"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Create Dataloaders for Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1671545902741,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"cLl5oQa5tm28","outputId":"b0829c1e-e868-4dc2-9b21-a2eaff99a7a6"},"outputs":[],"source":["# Create the dataloaders.\n","\n","train_data = CompDataset(df_mnli)\n","val_data = CompDataset(df_val)\n","test_data = TestDataset(df_test)\n","\n","train_dataloader = torch.utils.data.DataLoader(train_data,\n","                                        batch_size=BATCH_SIZE,\n","                                        shuffle=True,\n","                                       num_workers=NUM_CORES)\n","\n","val_dataloader = torch.utils.data.DataLoader(val_data,\n","                                        batch_size=BATCH_SIZE,\n","                                        shuffle=True,\n","                                       num_workers=NUM_CORES)\n","\n","test_dataloader = torch.utils.data.DataLoader(test_data,\n","                                        batch_size=BATCH_SIZE,\n","                                        shuffle=False,\n","                                       num_workers=NUM_CORES)\n","\n","\n","\n","print(len(train_dataloader))\n","print(len(val_dataloader))\n","print(len(test_dataloader))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1723316,"status":"ok","timestamp":1671547626047,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"l4x7Ge8jtm28","outputId":"39f98e86-83b0-4ba9-c628-a05df9813c89"},"outputs":[],"source":["# Set the seed.\n","seed_val = 400\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Store the average loss after each epoch so we can plot them.\n","loss_values = []\n","\n","\n","# For each epoch...\n","for epoch in range(0, NUM_EPOCHS):\n","    \n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch + 1, NUM_EPOCHS))\n","    \n","\n","    stacked_val_labels = []\n","    targets_list = []\n","\n","\n","# ========================================\n","    #               Training\n","    # ========================================\n","    \n","    print('Training on ...')\n","    print(model.device)\n","    \n","    # put the model into train mode\n","    model.train()\n","    \n","    # This turns gradient calculations on and off.\n","    torch.set_grad_enabled(True)\n","\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    for i, batch in enumerate(train_dataloader):\n","        \n","        train_status = 'Batch ' + str(i) + ' of ' + str(len(train_dataloader))\n","        \n","        print(train_status, end='\\r')\n","\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        model.zero_grad()\n","\n","        outputs = model(b_input_ids, \n","                    attention_mask=b_input_mask,\n","                    labels=b_labels)\n","        \n","        # Get the loss from the outputs tuple: (loss, logits)\n","        loss = outputs[0]\n","        \n","        # Convert the loss from a torch tensor to a number.\n","        # Calculate the total loss.\n","        total_train_loss = total_train_loss + loss.item()\n","        \n","        # Zero the gradients\n","        optimizer.zero_grad()\n","        \n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","        \n","        \n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        \n","        \n","        \n","        # Use the optimizer to update the weights.\n","        \n","        # Optimizer for GPU\n","        optimizer.step() \n","        \n","\n","\n","    \n","    print('Train loss:' ,total_train_loss)\n","\n","\n","\n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    \n","    print('\\nValidation...')\n","\n","    # Put the model in evaluation mode.\n","    model.eval()\n","\n","    # Turn off the gradient calculations.\n","    # This tells the model not to compute or store gradients.\n","    # This step saves memory and speeds up validation.\n","    torch.set_grad_enabled(False)\n","    \n","    \n","    # Reset the total loss for this epoch.\n","    total_val_loss = 0\n","    \n","\n","    for j, batch in enumerate(val_dataloader):\n","            \n","            val_status = 'Batch ' + str(j) + ' of ' + str(len(val_dataloader))\n","            \n","            print(val_status, end='\\r')\n","\n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)     \n","\n","\n","            outputs = model(b_input_ids, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels)\n","            \n","            # Get the loss from the outputs tuple: (loss, logits)\n","            loss = outputs[0]\n","            \n","            # Convert the loss from a torch tensor to a number.\n","            # Calculate the total loss.\n","            total_val_loss = total_val_loss + loss.item()\n","            \n","\n","            # Get the preds\n","            preds = outputs[1]\n","\n","\n","            # Move preds to the CPU\n","            val_preds = preds.detach().cpu().numpy()\n","            \n","            # Move the labels to the cpu\n","            targets_np = b_labels.to('cpu').numpy()\n","\n","            # Append the labels to a numpy list\n","            targets_list.extend(targets_np)\n","\n","            if j == 0:  # first batch\n","                stacked_val_preds = val_preds\n","            else:\n","                stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n","\n","        \n","        # Calculate the validation accuracy\n","    y_true = targets_list\n","    y_pred = np.argmax(stacked_val_preds, axis=1)\n","        \n","    val_acc = accuracy_score(y_true, y_pred)\n","        \n","        \n","    print('Val loss:' ,total_val_loss)\n","    print('Val acc: ', val_acc)\n","\n","\n","  # Use the garbage collector to save memory.\n","    gc.collect()\n","# Save the Model\n","torch.save(model.state_dict(), 'drive/MyDrive/NLP_Project/data/xlm_roberta_large_xnli_mlni_mnli_val_matched.pt')\n","\n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Predict Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55927,"status":"ok","timestamp":1671547681970,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"Shqyt17zC-L4","outputId":"9d3eb9e9-bffb-4bfa-ac7c-0fe8eb317e53"},"outputs":[],"source":["for j, batch in enumerate(test_dataloader):\n","        \n","        inference_status = 'Batch ' + str(j+1) + ' of ' + str(len(test_dataloader))\n","        \n","        print(inference_status, end='\\r')\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","\n","\n","        outputs = model(b_input_ids, \n","                attention_mask=b_input_mask)\n","        \n","        \n","        # Get the preds\n","        preds = outputs[0]\n","\n","\n","        # Move preds to the CPU\n","        preds = preds.detach().cpu().numpy()\n","        \n","        # Move the labels to the cpu\n","        targets_np = b_labels.to('cpu').numpy()\n","\n","        # Append the labels to a numpy list\n","        targets_list.extend(targets_np)\n","        \n","        # Stack the predictions.\n","\n","        if j == 0:  # first batch\n","            stacked_preds = preds\n","\n","        else:\n","            stacked_preds = np.vstack((stacked_preds, preds))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1671547681971,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"VlwPWZVRvgjX","outputId":"f51d71ac-64c8-48bf-ed76-20219f0f0c80"},"outputs":[],"source":["# Take the argmax. This returns the column index of the max value in each row.\n","\n","preds = np.argmax(stacked_preds, axis=1)\n","\n","preds"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"elapsed":461,"status":"ok","timestamp":1671547682426,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"TbQyrcZvvtGw","outputId":"c7289747-0dab-40b8-bc23-527fe9e233dd"},"outputs":[],"source":["path = 'drive/MyDrive/NLP_Project/data/sample_submission.csv'\n","\n","df_sample = pd.read_csv(path)\n","\n","print(df_sample.shape)\n","\n","df_sample.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1671547682427,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"4wCJ0FK4vwG1","outputId":"124a38ca-7f9b-4631-c84f-d86ac52e4ccd"},"outputs":[],"source":["df_sample['prediction'] = preds\n","df_sample['prediction'] = df_sample['prediction'].replace([0, 2], [2, 0])\n","\n","df_sample.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1671547682427,"user":{"displayName":"Dominik Bieri","userId":"16263014991290198195"},"user_tz":-60},"id":"M5WSPyiAvx9k"},"outputs":[],"source":["# Create a submission csv file\n","# Note that for this competition the submission file must be named submission.csv.\n","# Therefore, it won't be possible to submit this csv file for leaderboard scoring.\n","df_sample.to_csv(f'drive/MyDrive/NLP_Project/data/xlm roberta_large_mnli_submission.csv', index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"18A3R5Sg7jVgwi2PBFOBdPSmPeDZzLZNM","timestamp":1670753116409}]},"gpuClass":"premium","kernelspec":{"display_name":"nlpproject","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]"},"vscode":{"interpreter":{"hash":"1963e4c1efbc08bf37151025a3a70fa62b76f4594615bf06e73f232accade162"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"1c09cfb3782c4d51a3a65e88f23c664f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"25f3146c716d4c939540347cbf5eb73e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b1bacb8419c498d854d020c79847cc6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c83fce16b6214a5bbc79385f03961e5d","IPY_MODEL_e9e4f6ab9e364a2d9a61b784ecf52806","IPY_MODEL_8b2b05f0e4a743e7b67fd6e817eb3a5f"],"layout":"IPY_MODEL_7fb4ad8c04c942ee81a4c462e29ba9f0"}},"3d59e937083441c5b1f2592cd75ca9d4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59db2e01f17f4736b16b2a4d61253d59":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fb4ad8c04c942ee81a4c462e29ba9f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b2b05f0e4a743e7b67fd6e817eb3a5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d59e937083441c5b1f2592cd75ca9d4","placeholder":"​","style":"IPY_MODEL_dffd536873e04a3b915f816b2a322d66","value":" 5/5 [00:00&lt;00:00, 207.79it/s]"}},"c361381b324841e694dfcc68df691998":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c83fce16b6214a5bbc79385f03961e5d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59db2e01f17f4736b16b2a4d61253d59","placeholder":"​","style":"IPY_MODEL_c361381b324841e694dfcc68df691998","value":"100%"}},"dffd536873e04a3b915f816b2a322d66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9e4f6ab9e364a2d9a61b784ecf52806":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_25f3146c716d4c939540347cbf5eb73e","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1c09cfb3782c4d51a3a65e88f23c664f","value":5}}}}},"nbformat":4,"nbformat_minor":0}
