{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### source\n",
    "https://www.kaggle.com/code/alturutin/watson-xlm-r-nli-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add class for finding duplicates and deleting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuplicateDeleter:\n",
    "\n",
    "    def __init__(self, valid_set, external_dataset):\n",
    "        self.valid_set = valid_set\n",
    "        self.external_dataset = external_dataset\n",
    "\n",
    "    def preprocess_query(self, q):\n",
    "        punct = '[' + ''.join([c for c in string.punctuation if c != \"'\"]) + ']'\n",
    "        q = q.lower()\n",
    "        q = re.sub(punct, ' ', q)\n",
    "        q = re.sub('[ ]{2,}', ' ', q)\n",
    "        return q\n",
    "\n",
    "    def search_in_base(self, q, kb):\n",
    "        q = self.preprocess_query(q)\n",
    "        return int(q in kb)\n",
    "\n",
    "    def delete(self):\n",
    "        index_to_delete = []\n",
    "        original_length =  self.external_dataset.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "        self.external_dataset_preprocessed = self.external_dataset['premise'].apply(self.preprocess_query)  # preprocess the external dataset\n",
    "\n",
    "\n",
    "        \n",
    "        self.knowledge_base = set(self.external_dataset['premise'].apply(self.preprocess_query))            # create a set of the external dataset for searching duplicates\n",
    "        \n",
    "        self.valid_set['duplicate'] = self.valid_set['premise'].apply(lambda q: self.search_in_base(q, self.knowledge_base))    # search for duplicates in the valid set and mark them\n",
    "        print(f\"fraction of train set english premises occurence in MNLI = {self.valid_set.loc[self.valid_set.lang_abv=='en', 'duplicate'].mean() * 100}%\")\n",
    "\n",
    "        for i in self.valid_set[self.valid_set.duplicate > 0.5].index:\n",
    "\n",
    "            print(\"index from valid set to drop: \", i)\n",
    "\n",
    "            # search duplicates in external dataset\n",
    "            print(\"found in:\", self.external_dataset_preprocessed[self.external_dataset_preprocessed == self.preprocess_query(self.valid_set.iloc[i,1])].index)\n",
    "\n",
    "            for i in self.external_dataset_preprocessed[self.external_dataset_preprocessed == self.preprocess_query(self.valid_set.iloc[i,1])].index:\n",
    "                index_to_delete.append(i)\n",
    "            \n",
    "            # drop duplicates in external dataset\n",
    "            print(\"index in external dataset to drop: \", index_to_delete)\n",
    "\n",
    "        print(\"*******************************\")\n",
    "        print(set(index_to_delete))\n",
    "        print(\"index_to_delete\")\n",
    "        self.external_dataset.drop(set(index_to_delete), inplace=True)\n",
    "        \n",
    "        \n",
    "\n",
    "        print(original_length - self.external_dataset.shape[0], \" duplicates deleted\")\n",
    "        return self.external_dataset.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set = pd.read_csv(\"data/valid.csv\")\n",
    "valid_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"data/train.csv\")\n",
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(\"data/test.csv\")\n",
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_train_set = pd.concat([valid_set, train_set], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set_translated = pd.read_csv(\"data/valid.csv\")\n",
    "train_set_translated = pd.read_csv(\"data/train.csv\")\n",
    "val_train_set_translated = pd.concat([train_set_translated, valid_set_translated], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli = load_dataset('glue', 'mnli')\n",
    "df_mnli = pd.DataFrame.from_dict(mnli[\"train\"])\n",
    "#df_mnli = pd.DataFrame.from_dict(mnli[\"validation_matched\"])\n",
    "df_mnli.drop(columns=['idx'], inplace=True)\n",
    "original_count = df_mnli.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_deleter = DuplicateDeleter(val_train_set_translated, df_mnli)\n",
    "mnli_deleter.delete().to_csv(\"data/mnli_train.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli = load_dataset('snli')\n",
    "df_snli = pd.DataFrame.from_dict(snli[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_deleter = DuplicateDeleter(val_train_set_translated, df_snli)\n",
    "snli_deleter.delete().to_csv(\"data/snli_train.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnli_languages = ['ar', 'bg', 'de', 'el', 'en', 'es', 'fr', 'hi', 'ru', 'sw', 'th', 'tr', 'ur', 'vi', 'zh']\n",
    "df_ar = pd.DataFrame()\n",
    "df_bg = pd.DataFrame()\n",
    "df_de = pd.DataFrame()\n",
    "df_el = pd.DataFrame()\n",
    "df_en = pd.DataFrame() \n",
    "df_es = pd.DataFrame()\n",
    "df_fr = pd.DataFrame()\n",
    "df_hi = pd.DataFrame()\n",
    "df_ru = pd.DataFrame()\n",
    "df_sw = pd.DataFrame()\n",
    "df_th = pd.DataFrame()\n",
    "df_tr = pd.DataFrame()\n",
    "df_ur = pd.DataFrame()\n",
    "df_vi = pd.DataFrame()\n",
    "df_zh = pd.DataFrame()\n",
    "\n",
    "dataframes = [df_ar, df_bg, df_de, df_el, df_en, df_es, df_fr, df_hi, df_ru, df_sw, df_th, df_tr, df_ur, df_vi, df_zh]\n",
    "\n",
    "xnli_dataframe = pd.DataFrame()\n",
    "\n",
    "for i in range(len(xnli_languages)):\n",
    "    dataset = load_dataset('xnli', xnli_languages[i])\n",
    "    dataframes[i]= pd.DataFrame.from_dict(dataset[\"train\"])\n",
    "    xnli_dataframe = pd.concat([xnli_dataframe, dataframes[i]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnli_deleter = DuplicateDeleter(val_train_set, xnli_dataframe)\n",
    "xnli_deleted = xnli_deleter.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnli_deleted = xnli_deleted.sample(frac=1).reset_index(drop=True)\n",
    "xnli_deleted[0:500000].to_csv(\"data/xnli_train.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1963e4c1efbc08bf37151025a3a70fa62b76f4594615bf06e73f232accade162"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
